import numpy as np
from scipy.stats import chisquare

# Observed frequencies (from the candies you bought)
observed = np.array([12, 8, 10])   # Red=12, Green=8, Blue=10

# Expected frequencies (if equally distributed, each should be 30/3 = 10)
expected = np.array([10, 10, 10])

# Perform Chi-Square Goodness of Fit Test
chi2, p = chisquare(f_obs=observed, f_exp=expected)

print("Chi-Square Value:", chi2)
print("P-Value:", p)

# Decision at 5% significance level
if p < 0.05:
    print("Result: Reject Null Hypothesis → Distribution is NOT equal")
else:
    print("Result: Fail to Reject Null Hypothesis → Distribution is equal")

import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform

# Step 1: Create dataset (4 instances, 2 attributes)
data = np.array([
    [1, 2],  # Instance A
    [2, 4],  # Instance B
    [3, 6],  # Instance C
    [5, 8]   # Instance D
])

# Step 2: Compute pairwise Euclidean distances
dissimilarity = squareform(pdist(data, metric='euclidean'))

# Step 3: Convert to DataFrame for better display
instances = ["A", "B", "C", "D"]
dissimilarity_matrix = pd.DataFrame(dissimilarity, index=instances, columns=instances)

# Step 4: Display matrix
print("Dissimilarity Matrix (Euclidean distances):")
print(dissimilarity_matrix)

import pandas as pd
import numpy as np

# Step 1: Create Sales dataset
data = {
    'Weather': ['Sunny', 'Sunny', 'Rainy', 'Rainy', 'Sunny', 'Rainy'],
    'Weekend': ['Yes', 'No', 'Yes', 'No', 'Yes', 'No'],
    'Competitor': ['High', 'Low', 'Low', 'High', 'Low', 'Low'],
    'Discount': ['No', 'Yes', 'Yes', 'No', 'Yes', 'Yes']  # Target column
}
df = pd.DataFrame(data)

print("Sales Dataset:")
print(df)

# Step 2: Function to calculate entropy
def entropy(target_col):
    elements, counts = np.unique(target_col, return_counts=True)
    entropy_val = 0
    for i in range(len(elements)):
        prob = counts[i] / np.sum(counts)
        entropy_val -= prob * np.log2(prob)
    return entropy_val

# Step 3: Function to calculate Information Gain
def info_gain(data, split_attr, target_name="Discount"):
    # Total entropy
    total_entropy = entropy(data[target_name])

    # Values and counts for attribute
    vals, counts = np.unique(data[split_attr], return_counts=True)

    # Weighted entropy
    weighted_entropy = 0
    for i in range(len(vals)):
        subset = data[data[split_attr] == vals[i]]
        weighted_entropy += (counts[i] / np.sum(counts)) * entropy(subset[target_name])

    # Information Gain = total entropy - weighted entropy
    return total_entropy - weighted_entropy

# Step 4: ID3 Algorithm (recursive tree building)
def id3(data, originaldata, features, target_name="Discount", parent_node_class=None):
    # If all target values are same → return that value
    if len(np.unique(data[target_name])) <= 1:
        return np.unique(data[target_name])[0]

    # If dataset is empty → return majority class from original data
    elif len(data) == 0:
        return np.unique(originaldata[target_name])[np.argmax(
            np.unique(originaldata[target_name], return_counts=True)[1])]

    # If no features left → return parent class
    elif len(features) == 0:
        return parent_node_class

    # Otherwise, grow the tree
    else:
        # Majority class in current node
        parent_node_class = np.unique(data[target_name])[np.argmax(
            np.unique(data[target_name], return_counts=True)[1])]

        # Choose best feature by highest information gain
        gains = [info_gain(data, feature, target_name) for feature in features]
        best_feature = features[np.argmax(gains)]

        tree = {best_feature: {}}

        # Remove best feature from list
        remaining_features = [f for f in features if f != best_feature]

        # Split and recurse
        for value in np.unique(data[best_feature]):
            subset = data[data[best_feature] == value]
            subtree = id3(subset, data, remaining_features, target_name, parent_node_class)
            tree[best_feature][value] = subtree

        return tree

# Step 5: Run ID3 on Sales Dataset
features = list(df.columns[:-1])  # All except target
tree = id3(df, df, features)

print("\nDecision Tree (ID3):")
print(tree)


 
 